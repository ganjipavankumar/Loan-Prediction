View(train_data)
train_data$Gender <- factor(train_data$Gender, labels = c(0,1), levels = c('Female','Male'))
rm(r)
View(train_data)
str(train_data)
#Encoding Categorical variables into Numeric
train_data$Gender <- factor(train_data$Gender, labels = c(1,2), levels = c('Female','Male'))
train_data$Education <- factor(train_data$Education, labels = c(1,2), levels = c('Graduate','Not Graduate') )
any(is.na(train_data$Self_Employed))
train_data$Self_Employed <- factor(train_data$Self_Employed, labels = c(1,2,3), c('','Yes','No'))
train_data$Property_Area <- factor(train_data$Property_Area, labels = c(1,2), levels = c('Y','N'))
train_data <- fread('train_data.csv', stringsAsFactors = T)
View(train_data)
#importing data
test_data <- fread('test_data.csv', stringsAsFactors = T)
train_data <- fread('train_data.csv', stringsAsFactors = T)
#checking Missing values
sort(sapply(train_data, function(x) { sum(is.na(x)) }), decreasing=TRUE)
sort(sapply(test_data, function(x) { sum(is.na(x)) }), decreasing=TRUE)
#Imputing Missing Values for training set
library(mice)
imputed_data <- mice(train_data[,c(9,10,11)], m=5, maxit = 50, method = 'rf', seed = 500)
train_data[,c(9,10,11)] <- complete(imputed_data, 2)
#imputing Missing Values for test set
imputed_data <- mice(test_data[,c(9,10,11)], m=5, maxit = 50, method = 'rf', seed = 500)
test_data[,c(9,10,11)] <- complete(imputed_data, 2)
#Encoding Categorical variables into Numeric
train_data$Gender <- factor(train_data$Gender, labels = c(1,2), levels = c('Female','Male'))
train_data$Married <- factor(train_data$Married, labels = c(0,1), levels = c('No','Yes'))
train_data$Education <- factor(train_data$Education, labels = c(1,2), levels = c('Graduate','Not Graduate') )
str(train_data)
train_data$Self_Employed <- factor(train_data$Self_Employed, labels = c(1,2,3), c('','Yes','No'))
View(train_data)
train_data$Property_Area <- factor(train_data$Property_Area, labels = c(1,2,3), levels = c('Rural','Urban','Semiurban'))
train_data$Loan_Status <- factor(train_data$Loan_Status, labels = c(1,2), levels = c('Y','N') )
#Encoding Categorical variables into Numeric for test data
test_data$Gender <- factor(test_data$Gender, labels = c(1,2), levels = c('Female','Male'))
test_data$Married <- factor(test_data$Married, labels = c(0,1), levels = c('No','Yes'))
test_data$Education <- factor(test_data$Education, labels = c(1,2), levels = c('Graduate','Not Graduate') )
test_data$Self_Employed <- factor(test_data$Self_Employed, labels = c(1,2,3), c('','Yes','No'))
test_data$Property_Area <- factor(test_data$Property_Area, labels = c(1,2,3), levels = c('Rural','Urban','Semiurban'))
View(test_data)
install.packages("psych")
library(psych)
descride(train_data)
describe(train_data)
#set column level
levels(train_data$Dependents)[levels(train_data$Dependents) ==  "3+"] <- "3"
View(train_data)
#checking Missing values
sort(sapply(train_data, function(x) { sum(is.na(x)) }), decreasing=TRUE)
#Imputing Missing Value for Gender and Married
imputed_data <- mice(train_data[,c(2,3)], m=5, maxit = 50, method = 'rf', seed = 500)
#checking Missing values
sort(sapply(train_data, function(x) { sum(is.na(x)) }), decreasing=TRUE)
train_data[,c(2,3)] <- complete(imputed_data, 2)
#checking Missing values
sort(sapply(train_data, function(x) { sum(is.na(x)) }), decreasing=TRUE)
imputed_data <- mice(test_data[,c(2,3)], m=5, maxit = 50, method = 'rf', seed = 500)
test_data[,c(2,3)] <- complete(imputed_data, 2)
#checking Missing values
sort(sapply(test_data, function(x) { sum(is.na(x)) }), decreasing=TRUE)
#Feature Scaling
train_data$Gender <- scale(train_data$Gender)
#Feature Scaling
train_data$Gender <- as.integer(train_data$Gender)
train_data$Loan_ID <- as.integer(train_data$Loan_ID)
train_data[,c(3:11)] <- as.integer(train_data[,c(3:11)])
train_data$Married <- as.integer(train_data$Loan_ID)
#feature Scaling
train_data$Loan_ID <- scale(train_data$Loan_ID)
#Fitting training data to decision tree
library(rpart)
#Loan Prediction
#install.packages('data.table')
library(data.table)
#importing data
test_data <- fread('test_data.csv', stringsAsFactors = T)
train_data <- fread('train_data.csv', stringsAsFactors = T)
head(train_data)
sapply(train_data, class)
#checking Missing values
sort(sapply(train_data, function(x) { sum(is.na(x)) }), decreasing=TRUE)
sort(sapply(test_data, function(x) { sum(is.na(x)) }), decreasing=TRUE)
#Imputing Missing Values for training set
library(mice)
imputed_data <- mice(train_data[,c(9,10,11)], m=5, maxit = 50, method = 'rf', seed = 500)
train_data[,c(9,10,11)] <- complete(imputed_data, 2)
#imputing Missing Values for test set
imputed_data <- mice(test_data[,c(9,10,11)], m=5, maxit = 50, method = 'rf', seed = 500)
test_data[,c(9,10,11)] <- complete(imputed_data, 2)
#Encoding Categorical variables into Numeric for training data
train_data$Gender <- factor(train_data$Gender, labels = c(1,2), levels = c('Female','Male'))
train_data$Married <- factor(train_data$Married, labels = c(0,1), levels = c('No','Yes'))
train_data$Education <- factor(train_data$Education, labels = c(1,2), levels = c('Graduate','Not Graduate') )
train_data$Self_Employed <- factor(train_data$Self_Employed, labels = c(1,2,3), c('','Yes','No'))
train_data$Property_Area <- factor(train_data$Property_Area, labels = c(1,2,3), levels = c('Rural','Urban','Semiurban'))
train_data$Loan_Status <- factor(train_data$Loan_Status, labels = c(0,1), levels = c('N','Y') )
#Encoding Categorical variables into factors
test_data$Gender <- factor(test_data$Gender, labels = c(1,2), levels = c('Female','Male'))
test_data$Married <- factor(test_data$Married, labels = c(0,1), levels = c('No','Yes'))
test_data$Education <- factor(test_data$Education, labels = c(1,2), levels = c('Graduate','Not Graduate') )
test_data$Self_Employed <- factor(test_data$Self_Employed, labels = c(1,2,3), c('','Yes','No'))
test_data$Property_Area <- factor(test_data$Property_Area, labels = c(1,2,3), levels = c('Rural','Urban','Semiurban'))
#set column level
levels(train_data$Dependents)[levels(train_data$Dependents) ==  "3+"] <- "3"
#Imputing Missing Value for Gender and Married
imputed_data <- mice(train_data[,c(2,3)], m=5, maxit = 50, method = 'rf', seed = 500)
train_data[,c(2,3)] <- complete(imputed_data, 2)
imputed_data <- mice(test_data[,c(2,3)], m=5, maxit = 50, method = 'rf', seed = 500)
test_data[,c(2,3)] <- complete(imputed_data, 2)
#Converting categoical to integer for train data
train_data$Gender <- as.integer(train_data$Gender)
train_data$Loan_ID <- as.integer(train_data$Loan_ID)
train_data$Married <- as.integer(train_data$Married)
train_data$Dependents <- as.integer(train_data$Dependents)
train_data$Education <- as.integer(train_data$Education)
train_data$Self_Employed <- as.integer(train_data$Self_Employed)
train_data$Property_Area <- as.integer(train_data$Property_Area)
#train_data$Loan_Status <- as.integer(train_data$Loan_Status)
#Converting categorical to integer for test data
test_data$Gender <- as.integer(test_data$Gender)
test_data$Loan_ID <- as.integer(test_data$Loan_ID)
test_data$Married <- as.integer(test_data$Married)
test_data$Dependents <- as.integer(test_data$Dependents)
test_data$Education <- as.integer(test_data$Education)
test_data$Self_Employed <- as.integer(test_data$Self_Employed)
test_data$Property_Area <- as.integer(test_data$Property_Area)
#feature Scaling
train_data$Loan_ID <- scale(train_data$Loan_ID)
train_data$Gender <- scale(train_data$Gender)
train_data$Married <- scale(train_data$Married)
train_data$Dependents <- scale(train_data$Dependents)
train_data$Education <- scale(train_data$Education)
train_data$Self_Employed <- scale(train_data$Self_Employed)
train_data$ApplicantIncome <- scale(train_data$ApplicantIncome)
train_data$CoapplicantIncome <- scale(train_data$CoapplicantIncome)
train_data$LoanAmount <- scale(train_data$LoanAmount)
train_data$Loan_Amount_Term <- scale(train_data$Loan_Amount_Term)
train_data$Credit_History <- scale(train_data$Credit_History)
train_data$Property_Area <- scale(train_data$Property_Area)
test_data <- scale(test_data)
#Splitting train data in to two datasets for validation
library(caTools)
set.seed(123)
split <- sample.split(train_data$Loan_Status, SplitRatio = 0.80)
trainset <- subset(train_data, split == TRUE)
testset <- subset(train_data, split == FALSE)
View(train_data)
View(trainset)
#Fitting training data to KNN
library(class)
knn_classifier <- knn(train = trainset[,-13], test = testset[,-13], cl = trainset$Loan_Status, k=5, prob = TRUE)
#Fitting training data to SVM
library(e1071)
svm_classifier = svm(formula = Loan_Status ~ .,
data = trainset,
type = 'C-classification',
kernel = 'sigmoid')
#Fitting training data to naive bayes
library(e1071)
nb_classifier = naiveBayes(x = trainset[,-13],
y = trainset$Loan_Status)
#Fitting training data to decision tree
library(rpart)
dt_classifier <- rpart(formula = Loan_Status ~ ., data = train_data)
#Predicting the test results
svm_pred <- predict(svm_classifier, newdata = testset[,-13])
nb_pred <-  predict(nb_classifier, newdata = testset[,-13])
dt_pred <-  predict(dt_classifier, newdata = testset[,-13], type='class')
#confusion matrix
cm_knn <- table(testset$Loan_Status,knn_classifier)     #98 correct predictions 24 incorrect predictions
cm_svm <- table(testset$Loan_Status, svm_pred)          #101 correct predictions 21 incorrect predictions
cm_nb <- table(testset$Loan_Status, nb_pred)            #94 correct predictions 28 incorrect predictions
cm_knn
cm_svm
cm_nb
cm_dt <- table(testset$Loan_Status, dt_pred)
cm_dt
#Fitting training data to Random Forest
library(randomForest)
set.seed(123)
rf_classifier <- randomForest(x = trainset[,-13], y = trainset$Loan_Status, ntree = 500)
rf_pred <-  predict(rf_classifier, newdata = testset[,-13])
cm_rf <- table(testset$Loan_Status, rf_pred)
cm_rf
cm_svm
install.packages('OutlierD')
pairs(trainset)
#Fitting training data to XGBoost Model
library(xgboost)
xg_classifier <- xgboost(data = as.matrix(trainset[,-13]), label = trainset$Loan_Status, nrounds = 10)
xg_pred = predict(xg_classifier, newdata = as.matrix(testset[,-13]))
xg_pred = (xg_pred >= 0.5)
cm_xg <- table(testset$Loan_Status, xg_pred)
cm_xg
#Applying K-Fold cross Validation
# install.packages('caret')
library(caret)
head(trainset[-13])
head(trainset[,-13])
folds = createFolds(trainset$Loan_Status, k = 10)
cv = lapply(folds, function(x) {
training_fold = trainset[-x, ]
test_fold = trainset[x, ]
#paste classifier here
classifier = xgboost(data = as.matrix(trainset[,-13]), label = trainset$Loan_Status, nrounds = 10)
#paste predict here
y_pred = predict(xg_classifier, newdata = as.matrix(test_fold[-11]))
y_pred = (y_pred >= 0.5)
#Confusio Matrix
cm = table(test_fold[, 11], y_pred)
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
cv = lapply(folds, function(x) {
training_fold = trainset[-x, ]
test_fold = trainset[x, ]
#paste classifier here
classifier = xgboost(data = as.matrix(trainset[,-13]), label = trainset$Loan_Status, nrounds = 10)
#paste predict here
y_pred = predict(xg_classifier, newdata = as.matrix(test_fold[,-13]))
y_pred = (y_pred >= 0.5)
#Confusio Matrix
cm = table(test_fold$Loan_Status, y_pred)
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
cv = lapply(folds, function(x) {
training_fold = trainset[-x, ]
test_fold = trainset[x, ]
#paste classifier here
classifier = xgboost(data = as.matrix(trainset[,-13]), label = trainset$Loan_Status, nrounds = 10)
#paste predict here
y_pred = predict(xg_classifier, newdata = as.matrix(test_fold[,-13]))
y_pred = (y_pred >= 0.5)
#Confusio Matrix
cm = table(testset$Loan_Status, y_pred)
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
cv = lapply(folds, function(x) {
training_fold = trainset[-x, ]
test_fold = trainset[x, ]
#paste classifier here
classifier = xgboost(data = as.matrix(trainset[,-13]), label = trainset$Loan_Status, nrounds = 10)
#paste predict here
y_pred = predict(xg_classifier, newdata = as.matrix(testset[,-13]))
y_pred = (y_pred >= 0.5)
#Confusio Matrix
cm = table(testset$Loan_Status, y_pred)
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
folds = createFolds(trainset$Loan_Status, k = 10)
cv = lapply(folds, function(x) {
training_fold = trainset[-x, ]
test_fold = trainset[x, ]
#paste classifier here
classifier = xgboost(data = as.matrix(trainset[,-13]), label = trainset$Loan_Status, nrounds = 10)
#paste predict here
y_pred = predict(xg_classifier, newdata = as.matrix(testset[,-13]))
y_pred = (y_pred >= 0.5)
#Confusio Matrix
cm = table(testset$Loan_Status, y_pred)
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
cv = lapply(folds, function(x) {
training_fold = trainset[-x, ]
test_fold = trainset[x, ]
#paste classifier here
classifier = xgboost(data = as.matrix(trainset[,-13]), label = trainset$Loan_Status, nrounds = 10)
#paste predict here
y_pred = predict(xg_classifier, newdata = as.matrix(testset[,-13]))
y_pred = (y_pred >= 0.5)
#Confusio Matrix
cm = table(testset$Loan_Status, y_pred)
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
exit()
quickplot()
folds = createFolds(trainset$Loan_Status, k = 10)
cv = lapply(folds, function(x) {
training_fold = trainset[-x, ]
test_fold = trainset[x, ]
#paste classifier here
classifier = xgboost(data = as.matrix(trainset[,-13]), label = trainset$Loan_Status, nrounds = 10)
#paste predict here
xg_pred = predict(xg_classifier, newdata = as.matrix(testset[,-13]))
xg_pred = (xg_pred >= 0.5)
#Confusio Matrix
cm = table(testset$Loan_Status, xg_pred)
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
cm_xg
#Fitting training data to Gradient boost model
library(caret)
gb_classifier <- train(trainset$Loan_Status ~ ., data = trainset[,-13], method = "gbm", trControl = 'fitControl',verbose = FALSE)
gb_classifier <- train(trainset$Loan_Status ~ ., data = trainset[,-13], method = "gbm", trControl = 'fitControl',verbose = FALSE)
is.atomic(trainset$Loan_Status)
is.recursive(trainset$Loan_Status)
gb_classifier <- train(trainset['Loan_Status'] ~ ., data = trainset[,-13], method = "gbm", trControl = 'fitControl',verbose = FALSE)
gb_classifier <- train(trainset[,-13] ~ ., data = trainset[,-13], method = "gbm", trControl = 'fitControl',verbose = FALSE)
gb_classifier <- train(trainset[['Loan_Status']] ~ ., data = trainset[,-13], method = "gbm", trControl = 'fitControl',verbose = FALSE)
gb_classifier <- train(trainset[["Loan_Status"]] ~ ., data = trainset[,-13], method = "gbm", trControl = 'fitControl',verbose = FALSE)
is.recursive(trainset[['Loan_status']])
is.recursive(trainset[,13)
is.recursive(trainset[,13])
gb_classifier <- train(trainset[,13] ~ ., data = trainset[,-13], method = "gbm", trControl = 'fitControl',verbose = FALSE)
gb_classifier <- train(form = trainset[,13] ~ ., data = trainset[,-13], method = "gbm", trControl = 'fitControl',verbose = FALSE)
head(trainset[,13])
head(trainset['Loan_Status'])
head(trainset[['Loan_Status']])
gb_classifier <- train(form = trainset[['Loan_Status']] ~ ., data = trainset[,-13], method = "gbm", trControl = 'fitControl',verbose = FALSE)
gb_classifier <- train(form = trainset$Loan_Status ~ ., data = trainset[,-13], method = "gbm", trControl = 'fitControl',verbose = FALSE)
trainset$outcome1 <- ifelse(trainset$Loan_Status == 1, "Yes","No")
View(trainset)
gb_classifier <- train(form = trainset$outcome1 ~ ., data = trainset[,-14], method = "gbm", trControl = 'fitControl',verbose = FALSE)
trainset <- trainset[,-14]
trainset$outcome1 <- ifelse(trainset$Loan_Status == 1, "Yes","No")
trainset <- trainset[,-14]
gb_classifier <- train(form = Loan_Status ~ ., data = trainset[,-14], method = "gbm", trControl = 'fitControl',verbose = FALSE)
gb_classifier <- train(form = Loan_Status ~ ., data = trainset[,-13], method = "gbm", trControl = 'fitControl',verbose = FALSE)
View(trainset)
gb_classifier <- train(form = Loan_Status ~ ., data = trainset[,-13], method = "gbm", trControl = 'fitControl',verbose = FALSE)
gb_classifier <- train(form = Loan_Status ~ ., data = trainset, method = "gbm", trControl = 'fitControl',verbose = FALSE)
trainset$outcome1 <- ifelse(trainset$Loan_Status == 1, "Yes","No")
gb_classifier <- train(form = as.factor(outcome1) ~ ., data = trainset, method = "gbm", trControl = 'fitControl',verbose = FALSE)
trainset <- trainset[,-14]
gb_classifier <- train(form = Loan_Status ~ ., data = trainset, method = "gbm", trControl = 'fitControl',verbose = FALSE)
gb_classifier <- train(Loan_Status ~ ., data = trainset, method = "gbm", trControl = 'fitControl',verbose = FALSE)
gb_classifier <- gbm(Loan_Status ~ ., data = trainset,distribution = "gaussian",n.trees = 10000, interaction.depth = 4, shrinkage = 0.01)
library(gbm)
gb_classifier <- train(Loan_Status ~ ., data = trainset, method = "gbm", trControl = 'fitControl',verbose = FALSE)
gb_classifier <- gbm(Loan_Status ~ ., data = trainset,distribution = "gaussian",n.trees = 10000, interaction.depth = 4, shrinkage = 0.01)
gb_pred <- predict(gb_classifier, newdata = testset[,-13])
gb_pred <- predict(gb_classifier, newdata = testset[,-13], n.trees = n.trees)
n.trees = seq(from=100 ,to=10000, by=100)
gb_pred <- predict(gb_classifier, newdata = testset[,-13], n.trees = n.trees)
cm_gb <- table(testset$Loan_Status, gb_pred)
plot.gbm(gb_classifier)
gbm.perf(gb_classifier)
gb_pred
gb_pred[10000]
gb_pred[,10000]
#Visualisation of model for naive bayes
# Visualising the Training set results
library(ElemStatLearn)
#Applying kernel PCA for dimensinality reduction
# install.packages('kernlab')
library(kernlab)
kpca = kpca(~., data = trainset, kernel = 'rbfdot', features = 2)
training_set_pca = as.data.frame(predict(kpca, trainset))
training_set_pca$Purchased = trainset$Loan_Status
test_set_pca = as.data.frame(predict(kpca, testset))
test_set_pca$Purchased = testset$Loan_Status
View(test_set_pca)
training_set_pca <- training_set_pca[,-3]
View(training_set_pca)
training_set_pca$Loan_Status = trainset$Loan_Status
test_set_pca <- test_set_pca[,-3]
test_set_pca$Purchased = testset$Loan_Status
svm_classifier = svm(formula = Loan_Status ~ .,
data = training_set_pca,
type = 'C-classification',
kernel = 'sigmoid')
#Predicting the test results
svm_pred <- predict(svm_classifier, newdata = test_set_pca[,-3])
cm_svm <- table(test_set_pca$Loan_Status, svm_pred)          #99 correct predictions 24 incorrect predictions
View(test_set_pca)
test_set_pca <- test_set_pca[,-3]
View(test_set_pca)
test_set_pca$Loan_Status = testset$Loan_Status
View(test_set_pca)
cm_svm <- table(test_set_pca$Loan_Status, svm_pred)          #99 correct predictions 24 incorrect predictions
cm_svm
kpca = kpca(~., data = trainset, kernel = 'rbfdot', features = 8)
training_set_pca = as.data.frame(predict(kpca, trainset))
training_set_pca$Loan_Status = trainset$Loan_Status
test_set_pca = as.data.frame(predict(kpca, testset))
test_set_pca$Loan_Status = testset$Loan_Status
svm_classifier = svm(formula = Loan_Status ~ .,
data = training_set_pca,
type = 'C-classification',
kernel = 'sigmoid')
#Predicting the test results
svm_pred <- predict(svm_classifier, newdata = test_set_pca[,-3])
View(test_set_pca)
#Predicting the test results
svm_pred <- predict(svm_classifier, newdata = test_set_pca[,-9])
cm_svm <- table(test_set_pca$Loan_Status, svm_pred)          #99 correct predictions 24 incorrect predictions
cm_svm
kpca = kpca(~., data = trainset, kernel = 'rbfdot', features = 10)
training_set_pca = as.data.frame(predict(kpca, trainset))
training_set_pca$Loan_Status = trainset$Loan_Status
test_set_pca = as.data.frame(predict(kpca, testset))
test_set_pca$Loan_Status = testset$Loan_Status
svm_classifier = svm(formula = Loan_Status ~ .,
data = training_set_pca,
type = 'C-classification',
kernel = 'sigmoid')
#Predicting the test results
svm_pred <- predict(svm_classifier, newdata = test_set_pca[,-9])
#Predicting the test results
svm_pred <- predict(svm_classifier, newdata = test_set_pca[,-11])
cm_svm <- table(test_set_pca$Loan_Status, svm_pred)          #99 correct predictions 24 incorrect predictions
cm_svm
knn_classifier <- knn(train = training_set_pca[,-11], test = test_set_pca[,-11], cl = trainset$Loan_Status, k=5, prob = TRUE)
#Fitting training data to SVM
library(e1071)
svm_classifier = svm(formula = Loan_Status ~ .,
data = training_set_pca,
type = 'C-classification',
kernel = 'sigmoid')
#Fitting training data to naive bayes
library(e1071)
nb_classifier = naiveBayes(x = training_set_pca[,-11],
y = training_set_pca$Loan_Status)
#Fitting training data to decision tree
library(rpart)
dt_classifier <- rpart(formula = Loan_Status ~ ., data = training_set_pca)
#Fitting training data to Random Forest
library(randomForest)
set.seed(123)
rf_classifier <- randomForest(x = training_set_pca[,-11], y = training_set_pca$Loan_Status, ntree = 500)
#Fitting training data to XGBoost Model
library(xgboost)
xg_classifier <- xgboost(data = as.matrix(training_set_pca[,-11]), label = training_set_pca$Loan_Status, nrounds = 10)
#Fitting training data to Gradient boost model
#install.packages('gbm)
library(gbm)
gb_classifier <- gbm(Loan_Status ~ ., data = training_set_pca,distribution = "gaussian",n.trees = 10000, interaction.depth = 4, shrinkage = 0.01)
#Predicting the test results
svm_pred <- predict(svm_classifier, newdata = test_set_pca[,-11])
nb_pred <-  predict(nb_classifier, newdata = test_set_pca[,-11])
dt_pred <-  predict(dt_classifier, newdata = test_set_pca[,-11], type='class')
rf_pred <-  predict(rf_classifier, newdata = test_set_pca[,-11])
xg_pred <-  predict(xg_classifier, newdata = as.matrix(test_set_pca[,-11]))
xg_pred <- (xg_pred >= 0.5)
n.trees = seq(from=100 ,to=10000, by=100)
gb_pred <- predict(gb_classifier, newdata = test_set_pca[,-11], n.trees = n.trees)
#confusion matrix
cm_knn <- table(test_set_pca$Loan_Status,knn_classifier)     #102 correct predictions 20 incorrect predictions
cm_svm <- table(test_set_pca$Loan_Status, svm_pred)          #99 correct predictions 24 incorrect predictions
cm_nb <- table(test_set_pca$Loan_Status, nb_pred)            #103 correct predictions 19 incorrect predictions
cm_dt <- table(test_set_pca$Loan_Status, dt_pred)            #101 correct predictions 21 incorrect predictions
cm_rf <- table(test_set_pca$Loan_Status, rf_pred)            #98 correct predictions 24 incorrect predictions
cm_xg <- table(test_set_pca$Loan_Status, xg_pred)            #84 correct predictions 38 incorrect predictions
cm_knn
cm_svm
cm_nb
cm_dt
cm_rf
cm_xg
head(train_data)
# Applying k-Fold Cross Validation
# install.packages('caret')
library(caret)
folds <- createFolds(trainset$Loan_Status, k = 10)
cv <- lapply(folds, function(x) {
training_fold = trainset[-x, ]
test_fold = trainset[x, ]
classifier <- svm(formula = Loan_Status ~ .,
data = training_fold,
type = 'C-classification',
kernel = 'radial')
y_pred = predict(classifier, newdata = test_fold[-3])
cm = table(test_fold[, 3], y_pred)
accuracy <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
#Fitting training data to SVM
library(e1071)
cv <- lapply(folds, function(x) {
training_fold = trainset[-x, ]
test_fold = trainset[x, ]
classifier <- svm(formula = Loan_Status ~ .,
data = training_fold,
type = 'C-classification',
kernel = 'radial')
y_pred = predict(classifier, newdata = test_fold[-3])
cm = table(test_fold[, 3], y_pred)
accuracy <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return(accuracy)
})
sort(sapply(train_data, function(x) { sum(is.na(x)) }), decreasing=TRUE)
View(trainset)
# Applying k-Fold Cross Validation
set.seed(seed)
df <- trainset
n_train <- 100
xy <- gen_data(n_train, beta, sigma_eps)
#cross validation
train_control<- trainControl(method="cv", number=10, savePredictions = TRUE)
model<- train(Loan_Status~., data=trainset, trControl=train_control, method="rpart")
model$pred
model[,3]
model$pred[,3]
model
kf_pred <- predict(model, newdata = testset[,-13])
cm_kf <- table(testset$Loan_Status, kf_pred)
cm_kf
19+82
model<- train(Loan_Status~., data=train_set_pca, trControl=train_control, method="rpart")
model<- train(Loan_Status~., data=training_set_pca, trControl=train_control, method="rpart")
kf_pred <- predict(model, newdata = test_set_pca[,-11])
cm_kf <- table(test_set_pca$Loan_Status, kf_pred)
cm_kf
80+19
